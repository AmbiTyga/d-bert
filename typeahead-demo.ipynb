{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def predict_next(text, include_only=None, topk=10):\n",
    "    toks = torch.tensor(tokenizer.encode(text)).unsqueeze(0).cuda()\n",
    "    model_out = model(toks)[0][0, -1].view(-1)\n",
    "    if include_only:\n",
    "        include_ids = torch.tensor([tokenizer.encode(x)[0] for x in include_only])\n",
    "        model_include_only = model_out[:, include_ids].clone().detach()\n",
    "        model_out[:] = -1000\n",
    "        model_out[include_ids] = model_include_only        \n",
    "    indices = torch.sort(model_out, descending=True)[1][:topk].tolist()\n",
    "    return [tokenizer.decode([idx]) for idx in indices]\n",
    "\n",
    "def eval_specific(text, raw_toks):\n",
    "    eval_toks = list(map(tokenizer.encode, raw_toks))\n",
    "    toks = torch.tensor(tokenizer.encode(text)).unsqueeze(0).cuda()\n",
    "    model_out = model(toks)[0][0, -1].view(-1)\n",
    "    return sorted([(tok_text, model_out[tok[0]].item()) for tok_text, tok in zip(raw_toks, eval_toks)], \n",
    "                  key=lambda x: -x[1])\n",
    "\n",
    "def greedy_predict(text, n=10, sample=False, topk=10):\n",
    "    for _ in range(n):\n",
    "        next_ = predict_next(text, topk=topk)\n",
    "        text = text + (random.choice(next_) if sample else next_[0])\n",
    "    return text\n",
    "\n",
    "def beam_predict(text, n=10, w=4, active_h=1000):\n",
    "    open_set = [text]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was bad for me, it's bad enough for you. You can see how I'm feeling right now.\"<|endoftext|> \"And then it just hit home, because the way it was set and the fact that you can watch it for hours at an time was a big plus because there were no subtitles.\"\n",
      "\n",
      ",\n",
      ".<|endoftext|>\n",
      "\n",
      ", <</endofchapter>><|endoftext|> <</p> \"I don. Well.\"</<p>\"You know what? I don't think so.\" <span class=\">I think I'm going through the whole 'what the f-- is happening to you, and you don't understand' and then I don`T know.\"> <p> \"So you just don�T care, you know what? I'm not a fan and it�m really hard for me.\"<span id='favorit-2'>\n",
      ".\n",
      " (end) \"But you're a good guy!\"\" <p class==\"fan-link-1\\\" />\n",
      "\n",
      ". ( end )\n",
      ". (end of text)<|endoftext|> \"You can. But I think. You just aren`tee good enough.\"\n",
      " (p class='comment'/> <</span class> <\\/comment></html> <</span\n"
     ]
    }
   ],
   "source": [
    "# predict_next('http://youtube.com/user/')\n",
    "print(greedy_predict('The movie was bad', n=256, topk=5, sample=True))#, include_only='abcdefghijklmnopqrstuvwxyz')\n",
    "# eval_specific('Bill Gates created M', [' writer', ' artist', ' scientist', ' pianist', ' failure', ' politician', \n",
    "#                                    ' president', ' singer', ' director', ' success', ' genius', ' magnate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'Ġ1905',\n",
       " 'Ġserved',\n",
       " 'Ġto',\n",
       " 'Ġbe',\n",
       " 'Ġimportantly',\n",
       " 'Ġal',\n",
       " 'bert',\n",
       " 'Ġe',\n",
       " 'instein',\n",
       " 'er']"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'year 1905 served to be importantly albert einsteiner'\n",
    "[tokenizer.decoder[x] for x in tokenizer.encode(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ize'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder[1096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = list(model.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert.modeling_gpt2 import Conv1D\n",
    "# list(map(type, modules))\n",
    "attn_mods = list(filter(lambda x: isinstance(x, Conv1D), modules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85017600"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(x.numel() for x in y.parameters()) for y in attn_mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 768])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mods[3].weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attn_mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[198, 220, 198, 220, 198, 220, 198, 220, 198, 220, 198]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('\\n \\n \\n \\n \\n \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġ'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder[220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
